---
title: "Data imputation for EGSS"
autor: "Markus Fröhlich"
package: egssGF
abstract: >
  R-package providing gap-filling functionality for european environmental goods and services (EGSS) data. The methods for imputation of missing data are time series models for hierarchy levels with at least some reported data and mean imputation for hierarchy levels with no reported data. After the gap filling procedure resulting in a complete data set without any gaps the consistency of the data-set is established by an iterative proportional fitting approach. Original reported values are not changed by the fitting approach as long as the consistency in the original data set is fulfilled. If there is a contradiction in original reported data, aggregate values are protected to a higher degree than sub-aggregate values.
output: 
  BiocStyle::html_document:
    df_print: paged
    toc: true
    toc_float: true
vignette: >
  %\VignetteIndexEntry{egssGF}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", eval = TRUE
)
```

```{r setup,echo=FALSE,include=FALSE}
library(egssGF)
```

# Introduction

The environmental goods and services sector is sometimes called ‘eco-industries’ or ‘environmental
industry’. The ‘Employment Package’ launched in April 2012 identified the “green economy” as a key source
of job creation in Europe. The EGSS domain of the European Statistical System is the ideal framework to
collect data on employment that directly depends on the production of outputs intended to protect the
environment and to manage natural resources. Due to its compatibility with the boundaries and definitions
used in the national accounts the EGSS database is an indispensable input to microeconomic and
macroeconomic analysis of the green economy, environmental and resource policy analysis and the
monitoring of policy targets. For most of the countries the EGSS is important for analysing issues related to
green growth and green employment. The main demands for EGSS data come from various Commission
Directorates General and international organisations, national governments (e.g. ministries of environment,
finance and economy), but also from business associations, workers’ unions, environmental protection
agencies, non-profit organisations and the research community.

# Data sets
The program needs two input files for each run:

1. EGSS base data with periods from 2010 to year preceding the actual end ($t-1$)
2. National Account data with periods from 2010 to the actual end ($t$)

## EGSS Data

EGSS data is available in "MDT" flat format with the following file structure:

- *nace_r2*: Nace-Code with 42 levels
- *ceparema*: CEPA and CReMA-Codes with 29 levels: 
- *indic_pi*: Variables Output, Empoyees, Gross Value Added and Exports
- *ty*: Type of Variable (Market - Non-Market)
- *unit*
- *geo*: Country code - 26 EU-Countries plus Swizerland and Turkey (Malta and Cyprus are missing)
- *time*: Year of Observation
- *obs_value*: Observation Value
- *obs_status*
- *obs_conf*
- *obs_comment*

> Data Excerpt:

```{r,echo=FALSE}
data("dat_egssBas")
dat_egssBas
```

Missing values are empty cells which are replaced by "NA" (not available) in R. The data-set is not complete, i.e. not all possible combinations of the classification variables are available. Moreover, the structure of the data set is not consistent over periods, i.e. the structure for EGSS data before 2013 is different to the structure after 2012. This is true for division of type-variables. However, a complete and fully consistent data set (regarding the composition of the different hierarchies, not the observation values) is created with the function **loadEGSS** (see below). 

## Data from National Accounts

Data from national accounts is needed to calculate ratios of EGSS data to national accounts data. These ratios are extrapolated (back-, forecasted and interpolated) for years where no EGSS-information was provided by the member state. Estimated values for cells which have not been reported are then generated by multiplying these ratios with the respective national accounts data.

Data from national accounts is structured as follows:

- *time*: Year of Observation
- *geo*: Country code (28 EU countries)
- *indic_pi*: Variables Output, Empoyees, Gross Value Added and Exports
- *nace_r2*: Nace-Code with 42 levels
- *value*: Observation Value

The level of detail is predominantly the same as for EGSS-data. However, nace-codes E37, E38 and E39 are only available in aggregated form. Other differences in the Nace-structure:

- Nace-code C10-C12 is not available and will be imputed as missing.
- Nace-code E is missing - sum of E36 and E37-E39 is used


# Data processing
This is an explanation of all available functions provided in the *egssGF* Package. Most of these functions have to be executed in the right sequential order to perform gap filling.

## loadEGSS.R

The function **loadEGSS** converts the EGSS-flat file to an R data table. In a further step missing hierarchy levels are completed and the consistency of the data set concerning the division of the type variable is established. The following steps are performed:

1. Input File has to be available in the "MDT"-flat format
2. Standardisation of "ty"-variable:
     * create structure for "ty" according to requirements 2013 (levels: MKT, C_REP and ES_CS)
     * add a Rest Category: Rest=MKT-C_REP-ES_CS
     * transform data structure for periods before 2013 to this very structure, i.e.
            + C_REP=GDA+TCI (if both available "old" C_REP to delete)
            + Rest=GDC+TCE
3. Transfer "Basic Structure" to all Countries and Years (34.916 rows)
    *  4 variables (Production,Value Added,Employment and Exports)
    * 43 nace-codes (Nace-2digit levels, Nace-divisions and Totals)
    * 29 Ceparema codes (with aggregates)
    * 8 ty-codes and 4 ty-codes respectively (indic_pi="EXP")
4. Cells with Zeros and obs_status="L" (not applicable) are set to NA
5. Creation of a variable **orig** indicating if the observation value (obs_value) has been reported (orig=TRUE) This means that obs_value is other than missing. (all zeroes are taken as reported). If the variable obs_value is missing than orig is set to FALSE.

The output of this function is an R-data-table of complete EGSS data with all combinations for 1996 to the actual year for 28 countries - 26 EU-counties plus Swizerland and Turkey (Malta and Cyprus are missing).

### Arguments of the function

The function has only one argument (x) which is an R data table of the EGSS base flat file, which has to be available in .csv Format.^[The .csv-File is transformed to an R data table by the command fread("xxxx.csv").] 


## loadNA.R

Extrapolation of EGSS-data is not done on nominal data but based on ratios of nominal data to national account data. The extrapolated ratios are then applied to available national-account data. EGSS-ratios are calculated for each Year, Country and Variable in relation to (ranking according to Availability)

1. the total of the respective detailed NACE-Code (e.g. NACE-levels C31-C33, E36,...) per EU-country and year
      + NACE-Codes E37, E38 and E39 are not available for national accounts, therefore each of these
        codes is refered to the aggregate E37-E39 which is available at national account data
2. the total of the 1-digit NACE-Section per EU-country and year if national account data is not available at the detailed NACE-Code (e.g. NACE-levels A, C,...)
3. the total of all NACE-Codes of a specific Variable per EU-country and year (Output, Value added, Employment or Exports) (e.g. total national account value for Output)
4. the total of all (available) EU-countries per Variable (Output, Value added, Employment or Exports) and Year

Caution has to be exercised to calculate all the ratios for a specific aggregate (time series) based on the same reference level.

### Coding for hierarchy levels
For easier referencing of the various aggregation levels an aggregation code is created for every single aggregation level. This 17 digits-code is generated in the following manner (each element of the code is seperated by underscores):

1. two digits for country code followed by underscore (e.g. AT_,BE_,...)
2. three digits for variable (e.g. EMP_, OUT_,...)
3. three digits for NACE-level A00-U00: (e.g. A00_, E00_, E36_,...)
4. two digits for type:
    + 10_: anciliary
    + 20_: own use
    + 30_: non market
    + 40_: market
    + 41_: market ressource efficient
    + 42_: market environmental specific services
    + 43_: market rest category
5. three digits for Ceparema code    
    + 110-190: Environmental Protection Cepa 1-6
    + 200-260: Ressource Management CrEMa 10-16 -- with subcategories 111 and 112 (without 110) and 131,132,133 (without 130)

### Arguments of the function

The function has four arguments:

+ National accounts data set (argument x)
+ Enhanced EGSS-base data set (argument y)
+ Year for which flash estimates have to be generated (argument toEst)
+ Type variable, for which the data matrix should be estimated (argument t1, default TOT_EGSS)

# Data screening

## screenData.R

This function can be applied to the base data set (in unedited (not gap filled version) or edited (completed version) form). Plots are generated to display the completness level of different aggregated cells. Four different plots are available,

1. country times year
2. country times Nace
3. country times Ceparema
4. Nace times Ceparema


### Arguments of the function
The function offers nine arguments to choose from, which are:

+ Country code (argument g)
+ Year (argument per)
+ Variable (argument v; possible expressions: PROD, VA, EXP and EMP)
+ Nace code (argument n2)
+ Ceparema code (argument cep)
+ Ty-Variable (argument t1)
+ Kind of plot (argument p - see above)
+ Display parameter - Percentage of nonmissing values are displayed or Percentage of values > 0 are displayed(argument val)
+ Interactive graph (argument ia) - however, for this version no labels are available for the graph

> Example Country x Nace

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("Pictures/ScreenDataEx1.PNG")
```

## tables.R

This function produces a Nace x Ceparema Matrix with the indication which values have been reported by the country and which have not been reported. Additionally the marinal distribution of the table is compared with the sum of the values in each row/column. If the diff column and row are different from zero, there is no consistency between the column/row total and the sum of inividual row/column entries.

### Arguments of the function

The arguments of this funcion are:

+ data set which has to be desplayed (argument x)
+ period for which the data matrix should be shown (argument per)
+ country code (argument g)
+ variable code (argument v)

> Austrian production for 2015

```{r, echo=FALSE, out.width="100%", fig.align = "center"}
knitr::include_graphics("Pictures/TablesAT2015.PNG")
```

## tables1.R

This function visualizes in one table the consistency for all dimensions of the table, i.e. consistency of row totals to total total, consistency of colum total of TOT_CEPA and TOT_CREMA to total total, consistency of row sums of TOT_CEPA and TOT_CREMA to row-total, consistency of sum of CEPA1 to CEPA9 per row to TOT_CEPA of the respective row, consistency of fum of CREMA10 to CREMA16 per row to TOT_CREMA of the respective row, consistency of column totals of all ceparema codes (including TOT_CEPA and TOT_CREMA) to column totals.

### Arguments of the function

This function has the same arguments as tables.R.

> Austrian production for 2015

```{r, echo=FALSE, out.width="100%", fig.align = "center"}
knitr::include_graphics("Pictures/tables1.PNG")
```

# Methodology

## Innovations state space models for exponential smoothing^[taken from Hyndman, R.J., & Athanasopoulos, G. (2018) Forecasting: principles and practice, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on July 16th 2019.]
Exponential smoothing was proposed in the late 1950s^[Brown, 1959; Holt, 1957; Winters, 1960], and has motivated some of the most successful forecasting methods. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series, which is a great advantage and of major importance to applications in industry.

### Simple exponential smoothing
The simplest of the exponentially smoothing methods is naturally called simple exponential smoothing. This method is suitable for forecasting data with no clear trend or seasonal pattern.

For example, it may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages, where the weights decrease exponentially as observations come from further in the past — the smallest weights are associated with the oldest observations: 

$$y_{T+1|T}=\alpha y_T+\alpha(1-\alpha)y_{T-1}+\alpha(1-\alpha)^2 y_{T-2}+... $$
where $0≤α≤1$ is the smoothing parameter. The one-step-ahead forecast for time $T+1$ is a weighted average of all of the observations in the series $y_1,…,y_T$. The rate at which the weights decrease is controlled by the parameter $α$.

#### Component form

An alternative representation is the component form. For simple exponential smoothing, the only component included is the level, $ℓ_t$. Component form representations of exponential smoothing methods comprise a forecast equation and a smoothing equation for each of the components included in the method. The component form of simple exponential smoothing is given by: 

+ **Forecast equation** \ \ \ \ \ \ \     $y_{t+h|t}=ℓ_t$ \
+ **Smoothing equation** \ \ \ \    $ℓ_t=αy_t+(1−α)ℓ_{t−1}$, \

where $ℓ_t$ is the level (or the smoothed value) of the series at time t. Setting $h=1$ gives the fitted values, while setting $t=T$
gives the true forecasts beyond the training data.

The forecast equation shows that the forecast value at time $t+1$ is the estimated level at time $t$. The smoothing equation for the level (usually referred to as the level equation) gives the estimated level of the series at each period $t$.

### Trend methods

#### Holt's linear trend method

Holt (1957) extended simple exponential smoothing to allow the forecasting of data with a trend. This method involves a forecast equation and two smoothing equations (one for the level and one for the trend): 

+ **Forecast equation** \ \ \ \ $y_{t+h|t}=ℓ_t+hb_t$ \
+ **Level equation** \ \ \ \ \ \ \ \ \ $ℓ_t=αy_t+(1−α)(ℓ_{t−1}+b_{t−1})$ \
+ **Trend equation** \ \ \ \ \ \ \ \ \ $b_t=β∗(ℓ_t−ℓ_{t−1})+(1−β^∗)b_{t−1}$, \

where $ℓ_t$ denotes an estimate of the level of the series at time t, $b_t$ denotes an estimate of the trend (slope) of the series at time $t$, $α$ is the smoothing parameter for the level, $0≤α≤1$, and $β^∗$ is the smoothing parameter for the trend, $0≤β∗≤1$.

As with simple exponential smoothing, the level equation here shows that $ℓ_t$ is a weighted average of observation $y_t$ and the one-step-ahead training forecast for time $t$, here given by $ℓ_{t−1}+b_{t−1}$. The trend equation shows that $b_t$ is a weighted average of the estimated trend at time $t$ based on $ℓ_t−ℓ_{t−1}$ and $b_{t−1}$, the previous estimate of the trend.

The forecast function is no longer flat but trending. The h-step-ahead forecast is equal to the last estimated level plus h times the last estimated trend value. Hence the forecasts are a linear function of h. If we replace $ℓ_t$ with $y_{t+1|t}$ and $ℓ_{t−1}$ with $y_{t|t−1}$ in the smoothing equation, we will recover the weighted average form of simple exponential smoothing.

#### Damped trend methods

The forecasts generated by Holt’s linear method display a constant trend (increasing or decreasing) indefinitely into the future. Empirical evidence indicates that these methods tend to over-forecast, especially for longer forecast horizons. Motivated by this observation, Gardner & McKenzie (1985) introduced a parameter that “dampens” the trend to a flat line some time in the future. Methods that include a damped trend have proven to be very successful, and are arguably the most popular individual methods when forecasts are required automatically for many series.

In conjunction with the smoothing parameters $α$ and $β^∗$ (with values between 0 and 1 as in Holt’s method), this method also includes a damping parameter $0<ϕ<1$:

+ **Forecast equation** \ \ \ \ $y_{t+h|t}=ℓ_t+(ϕ+ϕ2+⋯+ϕh)b_t$  \
+ **Level equation** \ \ \ \ \ \ \ \ \ $ℓ_t=αy_t+(1−α)(ℓ_t−1+ϕb_{t−1})$ \
+ **Trend equation** \ \ \ \ \ \ \ \ \ $b_t=β^∗ (ℓ_t−ℓ_{t−1})+(1−β^∗)ϕb_{t−1}$ \

If $ϕ=1$, the method is identical to Holt’s linear method. For values between 0 and 1, $ϕ$ dampens the trend so that it approaches a constant some time in the future. In fact, the forecasts converge to $ℓ_T+ϕb_T/(1−ϕ)$ as $h→∞$ for any value $0<ϕ<1$. This means that short-run forecasts are trended while long-run forecasts are constant. In practice, $ϕ$ is rarely less than 0.8 as the damping has a very strong effect for smaller values. Values of $ϕ$ close to 1 will mean that a damped model is not able to be distinguished from a non-damped model. For these reasons, we usually restrict $ϕ$ to a minimum of 0.8 and a maximum of 0.98.

### Innovations state space models for exponential smoothing

The exponential smoothing methods presented above are algorithms which generate point forecasts. The statistical models here generate the same point forecasts, but can also generate prediction (or forecast) intervals. A statistical model is a stochastic (or random) data generating process that can produce an entire forecast distribution. 

Each model consists of a measurement equation that describes the observed data, and some state equations that describe how the unobserved components or states (level, trend) change over time. Hence, these are referred to as state space models.

For each method there exist two models: one with additive errors and one with multiplicative errors. The point forecasts produced by the models are identical if they use the same smoothing parameter values. They will, however, generate different prediction intervals. For further details to state space models for exponential smoothing refer to^[Hyndman, R.J., & Athanasopoulos, G. (2018)]

# Imputation

## extraPol.R

If at least one reported value is available for a specific hierarchy level (year x country x variable^[indic_pi]) we use exponetial smoothing state space models to estimate backwards and forwards. However, if only one observation is available or if the variance of all observations is equal to zero then this very one value is extrapolated to every period needed. Otherwise the exponential smoothing procedure **ETS** from the forecast package ^[Hyndman et al] is used to generate the estimated values. These models allow to include intercept terms, trend components and seasonal components. However, in our case no seasonal component was included because of the periodicity of the data (yearly). 

### Arguments of the function
The function has only one argument (x), which is the base level hierarchy^[Nace-codes x Ceparema codes without any aggregates, i.e. cells within the data matrix without row and column sums.] of the EGSS-data set enriched with national account data (output of function loadNA)


## gapFill.R

The imputation of missing values is done in two steps:

1. Extrapolation and Intrapolation of Levels for which at least one observation is available
2. Mean Imputation of Levels for which no observation is available

The gap filling for case 1 is done with the methodology described above. For case two the value of EGSS relative to national accounts is calculated for each year, country and variable. If for a certain hierarchy level no single value is available for the whole time span considered for a certain country and variable the median share of EGSS data to national accounts data for all available countries is imputed for the missing country^[Realiter, the number of nonmissing countries will differ from year to year. This will possibly effect the median value.]. This median share is then applied to the relevant national account data of the country considered. Data from national accounts must be available for every country, year and variable in order to produce a complete data matrix.

### Arguments of the function
The function has only one argument (x) beeing the EGSS-data set enriched with national account data (output of function loadNA).

## plotData.R 

The imputation of missing data is visualised with the function **plotData**. For each hierarchy level reported and estimated values can be compared with estimates from earlier versions of the data set. 

### Arguments of the function
The function has six arguments:

+ base data set (argument x)
+ edited data set (argument y)
+ country code (argument geoC; defaults to all countries)
+ Variable indic_pi (argument varC; defaults to Production)
+ Nace code (argument nC)
+ Ceparema code (argument cC)

In the plots data points from the x-data set are labled as "observed", those of the y-data set are labeled as "estimated". Therefore, if the x is set to the base data set with no gap filling, original reported values can be compared with any level of imputed data (original estimates vs. all levels of iterative fitted values).

> Example Plot - originally reported data with initial gap filled data

```{r, echo=FALSE, out.width="100%", fig.align = "center"}
knitr::include_graphics("Pictures/PlotData2.PNG")
```

# Consistency

The estimation of missing values in the gap filling procedure is done independently on each aggregation level. Therefore the consistency of the data set is not guaranteed. In fact, most aggregates will deviate from the sum of the respective sub-aggregates. In the next step the consistency of the data set is established in an iterative Procedure.

## iterFit.R

The function **iterFit** establishes consistency within the gap-filled data set. 

### Calculation of adjustment factors

In order to adjust each cell value in the Nace x Ceparema Matrix to the relevant marinal values, the subaggregates which have to sum to the aggregate are splitted into those values which have been reported by the country and values which have been estimated during the gap-filling procedure. The reason behind this splitting is, that original reported values should be kept unchanged during the adjustment algorithm. The following notation should facilitate the documentation:

+ sumEst: sum of estimated values for a specific aggregate
+ sumRep: sum of reported values for a specific aggregate (1 and 2 should sum up to the relevant aggregate value)
+ nEst: number of cells contributing to sumEst
+ nRep: number of cells contributing to sumRep
+ orig: indicator for the aggregate value - estimated or reported
+ obs_value: aggregate value (sumEst plus sumRep should sum up to obs_value)

### Conditions which have been established in order to receive consistancy:

- All zeroes of the data set were corrected to 0.0001
- The marker for all original reported zeroes in the data set was set to FALSE (i.e. these were set to not originally reported, so the propability of a change of these values was increased)

> Example

In the following table TOT_CEPA and TOT_CREMA for each Nace-Level are summed to fit the relevant total of TOT_CEPA and TOT_CREMA. In the first row the value for TOT_CEPA (lcode=xx_xxx_100, lcode=xx_xxx_200 for TOT_CREMA) for employment in Austria in the year 2010 for all Nace-codes was 91037.680. This value was originally reported by the country (orig=TRUE). From the 21 sub-aggregates 11 have been estimated (nEst=11) and for 10 Nace-codes the value for TOT_CEPA has been reported by the country. If the 11 estimated values and the 10 reported values are summed up the respective results are 37441.691 and 51892.570. The sum of these two values is 89334.26 which is slightly higher than the reported aggregat. Therefore the estimated values have to be downweighted. The original reported values stay unchanged as long as sumEst <= obs_value.

```{r, echo=FALSE, out.width="100%", fig.align = "center"}
knitr::include_graphics("Pictures/iterFitt.PNG")
```

### Current R-Code for adjustment within the function

`testAll[orig==FALSE & obs_value<sumRep, ":=" (obs_value = sumRep, korrObsV = 1)]`
`testAll[orig==FALSE & !is.na(sumEst) & !(sumEst>0) & sumRep>0, ":=" (obs_value = sumRep, korrObsV = 1)]`
`testAll[orig==TRUE & obs_value < 0.0002 & sumRep>0, ":=" (obs_value = sumRep, korrObsV = 1)]`

`testAll[is.na(sumEst) & is.na(sumRep),  ":=" (factorEst = 0, factorRep = 0)]`
`testAll[is.na(sumEst) & sumRep < 0,     ":=" (factorEst = 0, factorRep = 1)]` 
`testAll[is.na(sumEst) & sumRep == 0,    ":=" (factorEst = 0, factorRep = 1)]`
`testAll[is.na(sumEst) & sumRep > 0,     ":=" (factorEst = 0, factorRep = 1)]`
`testAll[is.na(sumEst) & sumRep > 0 & orig==TRUE & obs_value > 0,`
`                                         ":=" (factorEst = 0, factorRep = obs_value/sumRep)]`

`testAll[sumEst == 0 & is.na(sumRep),    ":=" (factorEst = 1, factorRep = 0)]`
`testAll[sumEst == 0 & sumRep < 0,       ":=" (factorEst = 1, factorRep = 0)]`
`testAll[sumEst == 0 & sumRep == 0,      ":=" (factorEst = 1, factorRep = 1)]`
`testAll[sumEst == 0 & sumRep > 0,       ":=" (factorEst = 1, factorRep = 1)]`
`testAll[sumEst == 0 & sumRep > 0 & orig == TRUE & obs_value > 0,`
`                                          ":=" (factorEst = 1, factorRep = obs_value/sumRep)]`

`testAll[!(sumEst == 0) & sumRep < 0,    ":=" (factorEst = (obs_value-sumRep)/sumEst, factorRep = 1)]`
`testAll[!(sumEst == 0) & sumRep == 0,   ":=" (factorEst = obs_value/sumEst, factorRep = 1)]`
`testAll[!(sumEst == 0) & is.na(sumRep), ":=" (factorEst = obs_value/sumEst, factorRep = 0)]`
`testAll[!(sumEst == 0) & sumRep > 0 & obs_value > 0 & obs_value >= sumRep,`
`                                          ":=" (factorEst = (obs_value-sumRep)/sumEst, factorRep = 1)]`

`testAll[!(sumEst == 0) & sumRep > 0 & obs_value > 0 & obs_value < sumRep & orig == TRUE,`
`                                          ":=" (factorEst = 0, factorRep = obs_value/sumRep)]`  

`testAll[!(sumEst == 0) & sumRep > 0 & obs_value > 0 & obs_value < sumRep & orig == FALSE,`
`                                          ":=" (factorEst = 1, factorRep = 1, obs_value = sum(sumRep, sumEst),`
`                                                korrObsV = 1)]`
`  testAll[sumEst > 0 & sumRep > 0 & obs_value == 0,`
`                                          ":=" (factorEst = 1, factorRep = 1, obs_value = sum(sumRep, sumEst),`
`                                                korrObsV = 1)]`
`  testAll[sumEst < 0 & sumRep > 0 & obs_value == 0,`
`                                          ":=" (factorEst = 0, factorRep = 1)]`  

### Arguments of the function

The function **iterFit** hast the following arguments:

- data set to be made consistent (argument x)
- argument for merging the sub-aggregate to the aggregate (argument z)
- code of hierarchy level to merge sub-aggregate values to relevent aggregate (argument lc)
- Nace-level of the aggregate (argument nLevel)
- Nace-level of the sub-aggregate (argument nLevel1)
- Ceparema-level of the aggregate (argument cLevel)
- Ceparema-level of the sub-aggregate (argument cLevel1)
- protection level of original reported values (argument mod)

## genConv.R

In order to generate a convergent data matrix the iterative fitting has to be executed in a specific order. The following eight iteration steps have to be processed:

1. fitting row totals to total of all cells
2. fitting column totals of TOT_CEPA and TOT_CREMA to total of all cells
3. fitting column totals of CEPA1-9 to column total of TOT_CEPA
4. fitting column totals of CREAM10-16 to column total of TOT_CREMA
5. fitting TOT_CEPA plus TOT_CREMA of each Nace Section to row total 
6. fitting sum of CEPA1-9 of each Nace Section to TOT_CEPA of respective Nace Section
7. fitting sum of CREMA10-16 of each Nace Section to TOT_CREMA of respective Nace Section
8. fitting column totals of CEPA1-9, TOT_CEPA, CREMA1-16 and TOT_CREMA to respective column totals

These iteration steps are performed in the function genConv. 

### Arguments of the function

The function has two arguments, the data set which should be made consistent and the number of iterations to perform. 

## iterControl.R

Function to check, if the iterative fitting was successful. All eight dimensions which are fitted are controlled. For each dimension the share of deviations > 0.5 and > 0.02 (in absolute value) are caluculated and the aggregation level with the highest absolute deviation is delivered.

### Arguments of the function
The argument (x) of this function is the data set which consistency has to be checked

## results.R

This function generates the final output table. The format is oriented on the input flat file.
